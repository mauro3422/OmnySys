# LFM2-Extract - Gu√≠a de Uso y Configuraci√≥n

## üìã Resumen

**LFM2-1.2B-Extract** es un modelo especializado de Liquid AI dise√±ado para extraer informaci√≥n estructurada de documentos no estructurados. Es 22.5x m√°s peque√±o que Gemma 3 27B pero con mejor performance en extracci√≥n.

### Caracter√≠sticas Principales

- **Tama√±o**: 1.2B par√°metros (~1.2GB en Q8_0)
- **Especializaci√≥n**: Extracci√≥n estructurada (JSON, XML, YAML)
- **Idiomas**: Ingl√©s, √Årabe, Chino, Franc√©s, Alem√°n, Japon√©s, Coreano, Portugu√©s, Espa√±ol
- **Performance**: Nivel GPT-4o en extracci√≥n (160x m√°s peque√±o)
- **Uso**: Single-turn conversations para extracci√≥n

## üéØ Casos de Uso

1. **Extracci√≥n de datos estructurados** de:
   - Documentos (art√≠culos, transcripciones, reportes)
   - C√≥digo fuente (localStorage keys, event names, API calls)
   - Emails (invoices, information extraction)

2. **Conversi√≥n de formatos**:
   - HTML ‚Üí JSON
   - Markdown ‚Üí Structured data
   - Plain text ‚Üí Schema-validated JSON

## üîß Configuraci√≥n

### System Prompt

El modelo **defaultea a JSON output** si no se proporciona system prompt. Para mejores resultados:

```json
{
  "systemPrompt": "Extract data and return as JSON with this schema:\n{\n  \"localStorage_keys\": [\"string\"],\n  \"event_names\": [\"string\"],\n  \"connections\": [{\"file\": \"string\", \"key\": \"string\"}]\n}"
}
```

### Par√°metros de Generaci√≥n

**CR√çTICO**: Usar **greedy decoding** para extracci√≥n:

```json
{
  "temperature": 0.0,
  "max_tokens": 1000,
  "stream": false
}
```

### Chat Template

LFM2-Extract usa formato ChatML:

```
<|startoftext|><|im_start|>system
[System prompt con schema JSON]<|im_end|>
<|im_start|>user
[Input document/code]<|im_end|>
<|im_start|>assistant
```

## üìù Best Practices para Prompting

### 1. Schema Expl√≠cito en System Prompt

‚ùå **MAL** (sin schema):
```
System: Extract localStorage keys from the code.
```

‚úÖ **BIEN** (con schema):
```
System: Extract localStorage keys and return as JSON:
{
  "localStorage_keys": ["string"],
  "connections": [{"source": "string", "target": "string", "key": "string"}]
}
```

### 2. Documentos Largos y Complejos

Para documentos >500 l√≠neas:
- Proporcionar schema m√°s detallado
- Especificar tipos de datos expl√≠citamente
- Incluir ejemplos en el system prompt

### 3. Single-Turn Conversations

El modelo est√° optimizado para una sola pregunta-respuesta:

```
User: [Document to extract from]
Assistant: {"extracted": "data"}
```

NO usar multi-turn o conversaciones largas.

### 4. Formato de Output

Especificar formato deseado:
- `"Return as JSON"`
- `"Return as XML"`
- `"Return as YAML"`

### 5. Cognitive Vaccines (Anti-Hallucination)

Incluir instrucciones expl√≠citas:

```
System: Extract ONLY information present in the input.
- DO NOT invent file names
- DO NOT assume connections
- COPY exact string literals
- If not found, return empty arrays
```

## üöÄ Integraci√≥n con llama.cpp

### Flags Importantes

```bash
llama-server \
  --model LFM2-1.2B-Extract-Q8_0.gguf \
  --port 8000 \
  --n-gpu-layers 999 \
  --parallel 4 \
  --ctx-size 32768 \
  --temp 0.0 \
  --json-schema-file extraction_schema.json  # ‚Üê CR√çTICO
```

### JSON Schema Enforcement

Crear `extraction_schema.json`:

```json
{
  "type": "object",
  "properties": {
    "localStorage_keys": {
      "type": "array",
      "items": {"type": "string"}
    },
    "event_names": {
      "type": "array",
      "items": {"type": "string"}
    },
    "connections": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "source": {"type": "string"},
          "target": {"type": "string"},
          "key": {"type": "string"}
        },
        "required": ["source", "target", "key"]
      }
    }
  },
  "required": ["localStorage_keys", "event_names", "connections"]
}
```

## üìä Performance Esperado

### Benchmarks

| Tarea | LFM2-Extract 1.2B | Gemma 3 27B | GPT-4o |
|-------|-------------------|-------------|--------|
| JSON Extraction | ‚úÖ Excelente | ‚úÖ Excelente | ‚úÖ Excelente |
| Multi-language | ‚úÖ 9 idiomas | ‚ö†Ô∏è Limitado | ‚úÖ 50+ idiomas |
| Speed | üöÄ ~700 tok/s | üêå ~50 tok/s | ‚è±Ô∏è Cloud API |
| Size | üíæ 1.2 GB | üíæ 27 GB | ‚òÅÔ∏è Cloud only |
| Cost | üí∞ FREE (local) | üí∞ FREE (local) | üí∞ API cost |

### Latency

- **Prompt eval**: ~1.4ms/token (GPU)
- **Generation**: ~37ms/token (GPU Vulkan RX 570)
- **Total** (500 token prompt + 100 token output): ~4.4 segundos

## üîÑ Migraci√≥n desde LFM2.5-Instruct

### Cambios Necesarios

1. **Modelo**: Cambiar de `-Instruct` a `-Extract`
2. **Temperature**: 0.1 ‚Üí 0.0
3. **System Prompt**: Agregar JSON schema expl√≠cito
4. **Thinking Tags**: Remover `<thinking>` tags (no necesarios)
5. **JSON Schema**: Agregar `--json-schema-file` flag

### C√≥digo de Ejemplo

**Antes** (Instruct):
```javascript
const prompt = `
Analyze this code for localStorage keys.

Code:
${code}

Return JSON.
`;
```

**Despu√©s** (Extract):
```javascript
const systemPrompt = `
Extract localStorage keys and return as JSON:
{"localStorage_keys": ["string"], "lines": [number]}
`;

const prompt = `
Code to analyze:
${code}
`;
```

## üêõ Troubleshooting

### Problema: Arrays vac√≠os en output

**Causa**: Schema no expl√≠cito o temperatura > 0

**Soluci√≥n**:
1. Agregar schema detallado en system prompt
2. Temperature = 0.0
3. Usar `--json-schema-file`

### Problema: JSON malformado

**Causa**: Sin JSON schema enforcement

**Soluci√≥n**:
```bash
llama-server --json-schema-file schema.json
```

### Problema: Inventa file names

**Causa**: Prompt sin cognitive vaccines

**Soluci√≥n**:
```
System: NEVER invent file names.
Use ONLY files mentioned in context.
If not found, return empty array.
```

## üìö Referencias

- [LFM2-1.2B-Extract (HuggingFace)](https://huggingface.co/LiquidAI/LFM2-1.2B-Extract)
- [LFM2-1.2B-Extract-GGUF](https://huggingface.co/LiquidAI/LFM2-1.2B-Extract-GGUF)
- [JSON Prompting Guide](https://codeconductor.ai/blog/structured-prompting-techniques-xml-json/)
- [Liquid AI Nanos Blog](https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices)

## ‚úÖ Checklist de Implementaci√≥n

- [ ] Descargar LFM2-1.2B-Extract-Q8_0.gguf
- [ ] Crear extraction_schema.json
- [ ] Actualizar ai-config.json con schema en systemPrompt
- [ ] Cambiar temperature a 0.0
- [ ] Remover `<thinking>` tags del prompt
- [ ] Agregar `--json-schema-file` flag al servidor
- [ ] Probar con scenario-4-localStorage-bridge
- [ ] Validar que arrays se llenan correctamente
