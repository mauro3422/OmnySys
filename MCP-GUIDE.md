# CogniSystem MCP - GuÃ­a de Uso

## ğŸš€ Comando Principal (Entry Point)

```bash
node src/layer-c-memory/mcp/index.js <ruta-del-proyecto>
```

### Ejemplos:

```bash
# Analizar proyecto de prueba
node src/layer-c-memory/mcp/index.js ./test-cases/scenario-ia-dynamic-imports

# Analizar tu propio proyecto
node src/layer-c-memory/mcp/index.js ./mi-proyecto

# Analizar proyecto con espacios (usar comillas)
node src/layer-c-memory/mcp/index.js "./mi proyecto"
```

## ğŸ“ Estructura del Comando

```
node src/layer-c-memory/mcp/index.js [opciones] <project-path>
```

| ParÃ¡metro | DescripciÃ³n | Requerido |
|-----------|-------------|-----------|
| `project-path` | Ruta al proyecto a analizar | âœ… SÃ­ |
| `--skip-llm` | Desactivar IA (solo anÃ¡lisis estÃ¡tico) | âŒ No |
| `--verbose` | Modo detallado | âŒ No |

## ğŸ”„ Flujo de EjecuciÃ³n (Arquitectura Actual)

Cuando ejecutÃ¡s el comando, el sistema hace automÃ¡ticamente:

```
1. Iniciar LLM Server (llama-server.exe en GPU)
   â””â”€ Espera a que estÃ© listo (health check)

2. Verificar AnÃ¡lisis Existente
   â””â”€ Si NO hay: Ejecutar Layer A (AnÃ¡lisis EstÃ¡tico)
   â””â”€ Si hay: Usar anÃ¡lisis existente

3. Inicializar Orchestrator
   â””â”€ Carga metadatos de Layer A
   â””â”€ Analiza quÃ© archivos necesitan LLM
   â””â”€ Agrega a cola de prioridad (background)

4. Inicializar Cache
   â””â”€ Carga anÃ¡lisis existente

5. Iniciar MCP Server
   â””â”€ Listo para recibir queries via stdio
   â””â”€ LLM processing continÃºa en background
```

### ğŸ—ï¸ Arquitectura de Capas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Server                                                  â”‚
â”‚  â””â”€ Entry point, recibe queries de la IA (Claude, etc.)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer A - Static Analysis                                   â”‚
â”‚  â””â”€ Parse AST, extrae metadatos (exports, imports, etc.)    â”‚
â”‚  â””â”€ NO usa LLM - es 100% estÃ¡tico                          â”‚
â”‚  â””â”€ Guarda en .OmnySysData/ (sin llmInsights)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestrator                                                â”‚
â”‚  â””â”€ Recibe metadatos de Layer A                             â”‚
â”‚  â””â”€ Decide quÃ© archivos necesitan LLM (basado en patrones)  â”‚
â”‚  â””â”€ Agrega a cola: critical > high > medium > low           â”‚
â”‚  â””â”€ Procesa en background (no bloquea al usuario)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis Worker                                             â”‚
â”‚  â””â”€ Toma job de la cola                                     â”‚
â”‚  â””â”€ Si needsLLM: usa LLMAnalyzer con prompt especializado   â”‚
â”‚  â””â”€ Guarda resultado CON llmInsights                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§  CuÃ¡ndo se activa el LLM

El LLM se activa para archivos que tienen:
- **Patrones complejos**: shared state (window.*), eventos, localStorage
- **Arquetipos detectados**: orphan-module, state-manager, god-object, etc.
- **Baja confianza**: conexiones detectadas con confianza < 0.8

El procesamiento es **asÃ­ncrono**: el usuario puede trabajar mientras el LLM analiza en background.

## ğŸ“Š QuÃ© obtienes

DespuÃ©s de ejecutar, se crea en tu proyecto:

```
tu-proyecto/
â”œâ”€â”€ .OmnySystemData/          â† Datos del anÃ¡lisis
â”‚   â”œâ”€â”€ index.json            â† Metadata general
â”‚   â”œâ”€â”€ files/                â† AnÃ¡lisis por archivo
â”‚   â”œâ”€â”€ connections/          â† Conexiones detectadas
â”‚   â””â”€â”€ risks/                â† EvaluaciÃ³n de riesgos
â”œâ”€â”€ system-map.json           â† Grafo de dependencias
â”œâ”€â”€ system-map-analysis.json  â† AnÃ¡lisis de calidad
â””â”€â”€ system-map-enhanced.json  â† AnÃ¡lisis semÃ¡ntico + IA
```

## ğŸ§  CuÃ¡ndo se activa la IA

La IA (Layer B) se activa automÃ¡ticamente cuando:

- Hay archivos "huÃ©rfanos" (sin imports ni usedBy)
- Se detecta cÃ³digo dinÃ¡mico (`import()`, `eval`)
- Hay eventos con nombres ambiguos
- Hay conexiones de baja confianza
- Archivos con side effects sospechosos

## ğŸ› ï¸ Troubleshooting

### "Found 0 JS/TS files"
Verificar que la ruta sea correcta y existan archivos `.js` o `.ts`

### "LLM server not available"
- Verificar que `llama-server.exe` estÃ© en `src/ai/server/`
- Verificar que el modelo `.gguf` estÃ© en `src/ai/models/`
- Verificar que no haya otro proceso usando el puerto 8000

### "Out of memory"
- El modelo Q8_0 usa ~1.2GB VRAM
- Si tu GPU tiene menos de 2GB, usar CPU mode (editar config)

## ğŸ“ Para otras IAs (Claude, GPT, etc.)

Esta es la forma de integrar CogniSystem:

```bash
# 1. Clonar/instalar CogniSystem
git clone <repo>
cd OmnySystem
npm install

# 2. Ejecutar MCP server en el proyecto objetivo
node src/layer-c-memory/mcp/index.js ./ruta-del-proyecto

# 3. El servidor queda escuchando en stdio
# Las herramientas disponibles son:
# - get_impact_map(filePath)
# - analyze_change(filePath, symbol)
# - explain_connection(fileA, fileB)
# - get_risk_assessment(minSeverity)
# - search_files(pattern)
```

## ğŸ”— IntegraciÃ³n MCP Protocol

El servidor implementa el [Model Context Protocol](https://modelcontextprotocol.io/):

```json
{
  "tools": [
    {
      "name": "get_impact_map",
      "description": "Returns impact analysis for a file",
      "parameters": {
        "filePath": "string"
      }
    },
    {
      "name": "analyze_change",
      "description": "Analyzes impact of changing a symbol",
      "parameters": {
        "filePath": "string",
        "symbolName": "string"
      }
    }
  ]
}
```

## ğŸ“ˆ Performance Esperado

| Proyecto | Tiempo Layer A | Tiempo Layer B (IA) | Total |
|----------|---------------|---------------------|-------|
| 10 archivos | 5-10s | 10-20s | 15-30s |
| 50 archivos | 20-30s | 30-60s | 50-90s |
| 100 archivos | 40-60s | 60-120s | 100-180s |

*Layer B solo corre en ~20% de archivos (los complejos)*

---

**Para mÃ¡s info**: Ver `test-cases/IA-TEST-GUIDE.md` para casos de prueba especÃ­ficos.
