/**
 * @fileoverview Batch Processor - Procesamiento batch de peticiones LLM
 * 
 * Responsabilidad √önica (SRP): Procesar m√∫ltiples peticiones LLM en batch.
 * 
 * @module llm-service/batch
 */

import { createLogger } from '../../../utils/logger.js';

const logger = createLogger('OmnySys:services:llm:batch');

/**
 * Resultado de procesamiento batch
 * @typedef {Object} BatchResult
 * @property {Array} results - Resultados de cada petici√≥n
 * @property {number} successful - Cantidad de peticiones exitosas
 * @property {number} failed - Cantidad de peticiones fallidas
 * @property {number} duration - Duraci√≥n total en ms
 */

/**
 * Procesa m√∫ltiples peticiones en batch
 * 
 * @param {Array<{prompt: string, options?: object}>} requests - Peticiones a procesar
 * @param {Function} analyzeFn - Funci√≥n de an√°lisis individual
 * @param {Object} options - Opciones de procesamiento
 * @param {number} options.concurrency - Concurrencia (default: 1)
 * @returns {Promise<BatchResult>} Resultados del procesamiento
 */
export async function processBatch(requests, analyzeFn, options = {}) {
  const startTime = Date.now();
  const concurrency = options.concurrency || 1;
  
  logger.info(`üîÑ Processing batch of ${requests.length} requests (concurrency: ${concurrency})`);
  
  let successful = 0;
  let failed = 0;
  
  let results;
  if (concurrency === 1) {
    results = await processSequential(requests, analyzeFn, () => successful++, () => failed++);
  } else {
    results = await processConcurrent(requests, analyzeFn, concurrency, () => successful++, () => failed++);
  }
  
  const duration = Date.now() - startTime;
  
  logger.info(`‚úÖ Batch completed: ${successful} successful, ${failed} failed (${duration}ms)`);
  
  return {
    results,
    successful,
    failed,
    duration
  };
}

/**
 * Procesa peticiones secuencialmente
 * @private
 */
async function processSequential(requests, analyzeFn, onSuccess, onFailure) {
  const results = [];
  
  for (const request of requests) {
    try {
      const result = await analyzeFn(request.prompt, request.options);
      results.push(result);
      onSuccess();
    } catch (error) {
      results.push({ error: error.message });
      onFailure();
    }
  }
  
  return results;
}

/**
 * Procesa peticiones concurrentemente
 * @private
 */
async function processConcurrent(requests, analyzeFn, concurrency, onSuccess, onFailure) {
  const results = [];
  const chunks = chunkArray(requests, concurrency);
  
  for (const chunk of chunks) {
    const chunkResults = await Promise.all(
      chunk.map(req => 
        analyzeFn(req.prompt, req.options)
          .then(result => {
            onSuccess();
            return result;
          })
          .catch(err => {
            onFailure();
            return { error: err.message };
          })
      )
    );
    results.push(...chunkResults);
  }
  
  return results;
}

/**
 * Divide un array en chunks
 * @param {Array} array - Array a dividir
 * @param {number} chunkSize - Tama√±o de cada chunk
 * @returns {Array<Array>} Array de chunks
 */
export function chunkArray(array, chunkSize) {
  const chunks = [];
  for (let i = 0; i < array.length; i += chunkSize) {
    chunks.push(array.slice(i, i + chunkSize));
  }
  return chunks;
}

/**
 * Calcula el tama√±o √≥ptimo de chunk basado en el n√∫mero de requests
 * @param {number} totalRequests - Total de peticiones
 * @param {number} maxConcurrency - Concurrencia m√°xima deseada
 * @returns {number} Tama√±o √≥ptimo de chunk
 */
export function calculateOptimalChunkSize(totalRequests, maxConcurrency) {
  if (totalRequests <= maxConcurrency) {
    return totalRequests;
  }
  
  // Distribuir uniformemente
  const numChunks = Math.ceil(totalRequests / maxConcurrency);
  return Math.ceil(totalRequests / numChunks);
}
