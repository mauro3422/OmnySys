/**
 * @fileoverview llm-starter.js
 * 
 * Inicia el servidor LLM (GPU y/o CPU) y espera a que est√© listo.
 * 
 * @module mcp/core/llm-starter
 */

import path from 'path';
import fs from 'fs/promises';
import { spawn } from 'child_process';
import { fileURLToPath } from 'url';
import { LLMClient, loadAIConfig } from '#ai/llm-client.js';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
import { createLogger } from '../../../utils/logger.js';

const logger = createLogger('OmnySys:llm:starter');



function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

/**
 * Espera a que el LLM est√© listo (health check)
 */
async function waitForLLM(client, maxRetries = 60, retryDelay = 2000) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const health = await client.healthCheck();
      if (health.gpu || health.cpu) {
        return { ready: true, health };
      }
    } catch { }
    process.stderr.write('.');
    await sleep(retryDelay);
  }
  return { ready: false };
}

/**
 * Inicia el servidor LLM (GPU y/o CPU seg√∫n configuraci√≥n)
 * 
 * @param {string} OmnySysRoot - Ruta ra√≠z del proyecto
 * @returns {Promise<boolean>} - true si est√° listo
 */
export async function startLLM(OmnySysRoot) {
  logger.info('   üîç Checking LLM status...');

  const aiConfig = await loadAIConfig();
  const client = new LLMClient(aiConfig);

  // 1. Check if already running
  try {
    const health = await client.healthCheck();
    if (health.gpu || health.cpu) {
      const mode = health.gpu ? 'GPU' : 'CPU';
      logger.info(`   ‚úì LLM already running on port 8000 (${mode} mode)`);
      return true;
    }
  } catch { }

  if (!aiConfig.llm.enabled) {
    logger.info('   ‚ÑπÔ∏è  LLM disabled in config');
    return false;
  }

  // 2. Clean stale lock file
  const lockFile = path.join(process.env.TEMP || '/tmp', 'omny_brain_gpu.lock');
  try { await fs.unlink(lockFile); } catch { }

  const mode = aiConfig.llm.mode || 'gpu';
  const scriptPath = path.join(OmnySysRoot, 'src/ai/scripts');

  // 3. Start GPU server if configured
  if (mode === 'gpu' || mode === 'both') {
    const gpuScript = path.join(scriptPath, 'brain_gpu.bat');

    try {
      await fs.access(gpuScript);
      logger.info('   üöÄ Starting GPU server...');

      const gpuProcess = spawn('cmd.exe', ['/c', 'start', gpuScript], {
        detached: true,
        stdio: 'ignore',
        windowsHide: true
      });
      gpuProcess.unref();

      logger.info('   ‚úì GPU server starting (port 8000)...');
    } catch {
      logger.info('   ‚ö†Ô∏è  GPU script not found');
    }
  }

  // 4. Start CPU server if configured
  if (mode === 'cpu' || mode === 'both') {
    const cpuScript = path.join(scriptPath, 'start_brain_cpu.bat');

    try {
      await fs.access(cpuScript);
      logger.info('   üöÄ Starting CPU server...');

      const cpuProcess = spawn('cmd.exe', ['/c', 'start', cpuScript], {
        detached: true,
        stdio: 'ignore',
        windowsHide: true
      });
      cpuProcess.unref();

      logger.info('   ‚úì CPU server starting (port 8002)...');
    } catch {
      logger.info('   ‚ö†Ô∏è  CPU script not found');
    }
  }

  // 5. Wait for LLM to be ready
  logger.info('   ‚è≥ Waiting for LLM to be ready (this may take 10-30s)...');

  const result = await waitForLLM(client, 60, 2000); // 2 min max

  if (result.ready) {
    const activeMode = result.health.gpu ? 'GPU' : 'CPU';
    logger.info(`\n   ‚úÖ LLM is ready! (${activeMode} mode)`);
    return true;
  } else {
    logger.info('\n   ‚ùå LLM failed to start within 2 minutes');
    logger.info('   üí° Check the terminal windows for errors');
    return false;
  }
}

/**
 * Inicia el servidor LLM en BACKGROUND sin esperar (non-blocking)
 * Usado durante la inicializaci√≥n del pipeline MCP
 * 
 * @param {string} OmnySysRoot - Ruta ra√≠z del proyecto
 * @returns {Promise<boolean>} - true si se inici√≥ el proceso (no garantiza que est√© listo)
 */
export async function startLLMBackground(OmnySysRoot) {
  logger.info('   üîç Checking LLM status...');

  const aiConfig = await loadAIConfig();
  const client = new LLMClient(aiConfig);

  // 1. Check if already running
  try {
    const health = await client.healthCheck();
    if (health.gpu || health.cpu) {
      const mode = health.gpu ? 'GPU' : 'CPU';
      logger.info(`   ‚úì LLM already running on port 8000 (${mode} mode)`);
      return true;
    }
  } catch { }

  if (!aiConfig.llm.enabled) {
    logger.info('   ‚ÑπÔ∏è  LLM disabled in config');
    return false;
  }

  // 2. Clean stale lock file
  const lockFile = path.join(process.env.TEMP || '/tmp', 'omny_brain_gpu.lock');
  try { await fs.unlink(lockFile); } catch { }

  const mode = aiConfig.llm.mode || 'gpu';
  const scriptPath = path.join(OmnySysRoot, 'src/ai/scripts');
  let started = false;

  // 3. Start GPU server if configured
  if (mode === 'gpu' || mode === 'both') {
    const gpuScript = path.join(scriptPath, 'brain_gpu.bat');

    try {
      await fs.access(gpuScript);
      logger.info('   üöÄ Starting GPU server...');

      const gpuProcess = spawn('cmd.exe', ['/c', 'start', gpuScript], {
        detached: true,
        stdio: 'ignore',
        windowsHide: true
      });
      gpuProcess.unref();

      logger.info('   ‚úì GPU server starting (port 8000)...');
      started = true;
    } catch {
      logger.info('   ‚ö†Ô∏è  GPU script not found');
    }
  }

  // 4. Start CPU server if configured
  if (mode === 'cpu' || mode === 'both') {
    const cpuScript = path.join(scriptPath, 'start_brain_cpu.bat');

    try {
      await fs.access(cpuScript);
      logger.info('   üöÄ Starting CPU server...');

      const cpuProcess = spawn('cmd.exe', ['/c', 'start', cpuScript], {
        detached: true,
        stdio: 'ignore',
        windowsHide: true
      });
      cpuProcess.unref();

      logger.info('   ‚úì CPU server starting (port 8002)...');
      started = true;
    } catch {
      logger.info('   ‚ö†Ô∏è  CPU script not found');
    }
  }

  // 5. Return immediately - don't wait for LLM to be ready
  if (started) {
    logger.info('   üöÄ LLM processes started in background');
    logger.info('   ‚è≥ Will be ready in 10-30 seconds...');
  }

  return started;
}

/**
 * Verifica si el LLM est√° disponible sin intentar iniciarlo
 */
export async function checkLLMStatus() {
  try {
    const aiConfig = await loadAIConfig();
    const client = new LLMClient(aiConfig);
    const health = await client.healthCheck();
    return {
      available: health.gpu || health.cpu,
      gpu: health.gpu,
      cpu: health.cpu
    };
  } catch {
    return { available: false, gpu: false, cpu: false };
  }
}
