# Checkpoint - Sistema CogniSystem MCP

## ğŸ“… Fecha: 2026-02-05
## ğŸ¯ Estado: 80% completado - Sistema funcionando con LLM en Orchestrator

---

## âœ… LO QUE FUNCIONA:

### 1. Arquitectura implementada:
- **Layer A**: AnÃ¡lisis estÃ¡tico puro (sin LLM) - genera metadatos
- **Orchestrator**: Recibe metadatos, decide quÃ© archivos necesitan LLM
- **Cola de prioridad**: Procesa archivos en orden (critical > high > medium > low)
- **Worker**: Usa LLM cuando `needsLLM: true`, guarda con `llmInsights`
- **MCP**: No bloquea, permite trabajar mientras procesa en background

### 2. Flujo de datos:
```
Layer A (indexer.js)
  â†“ Genera metadatos
  â†“ Guarda en .OmnySysData/ (sin llmInsights)
  
Orchestrator (orchestrator.js)
  â†“ Lee metadatos
  â†“ Detecta arquetipos (detectArchetypes)
  â†“ Decide quÃ© necesita LLM
  â†“ Agrega a cola (enqueueJob)
  
AnalysisWorker (analysis-worker.js)
  â†“ Procesa job
  â†“ Si needsLLM: usa LLMAnalyzer
  â†“ Guarda resultado CON llmInsights
```

### 3. Templates de prompts:
- `semantic-connections.js`: Con CoT (Chain of Thought) - MODIFICADO
- `orphan-module.js`: BÃ¡sico
- `god-object.js`: BÃ¡sico
- `dynamic-imports.js`: BÃ¡sico
- `global-state.js`: BÃ¡sico

### 4. Problema identificado:
Los prompts actuales son **especialistas** (uno por patrÃ³n), pero un archivo puede tener mÃºltiples patrones. Esto causa:
- AnÃ¡lisis incompleto (solo ve un aspecto)
- Razonamientos genÃ©ricos
- No detecta conexiones entre patrones

---

## ğŸ”§ ARCHIVOS MODIFICADOS:

1. `src/layer-a-static/indexer.js`
   - Removido LLM enrichment de generateEnhancedSystemMap
   - Layer A solo hace anÃ¡lisis estÃ¡tico

2. `src/core/orchestrator.js`
   - Agregado `_analyzeComplexFilesWithLLM()`
   - Agregado `_calculateLLMPriority()`
   - Modificado `_processNext()` para loop continuo
   - Worker procesa con LLM cuando needsLLM

3. `src/core/analysis-worker.js`
   - Modificado `analyze()` para usar LLMAnalyzer cuando needsLLM
   - Merge de resultado LLM con anÃ¡lisis estÃ¡tico

4. `src/core/analysis-queue.js`
   - Agregado `enqueueJob()` para aceptar objetos completos

5. `src/layer-b-semantic/prompt-engine/prompt-templates/semantic-connections.js`
   - Agregado Chain of Thought (CoT)
   - Mejorado reasoning con pasos especÃ­ficos

6. `src/layer-b-semantic/llm-analyzer.js`
   - Modificado `normalizeResponse()` para preservar campos originales

7. `src/layer-a-static/storage/storage-manager.js`
   - Modificado `saveFileAnalysis()` para merge preservando llmInsights

8. `src/layer-c-memory/mcp/server.js`
   - Modificado `checkAndRunAnalysis()` para no bloquear
   - Agregado `_countPendingLLMAnalysis()`

---

## ğŸ§ª PRUEBA PENDIENTE:

**Objetivo**: Verificar si un prompt **comprehensivo** (Ãºnico) puede extraer todos los patrones de un archivo de una sola vez, en lugar de usar mÃºltiples prompts especialistas.

**Ventaja**: Un solo llamado a LLM por archivo (mÃ¡s rÃ¡pido, menos tokens)

**Script de prueba**: `test-comprehensive-prompt.js`

---

## ğŸ¯ SIGUIENTES PASOS:

1. âœ… Ejecutar prueba del prompt comprehensivo
2. âœ… Evaluar resultados vs prompts especialistas
3. âœ… Decidir: Â¿un prompt Ãºnico o mÃºltiples prompts?
4. âœ… Actualizar templates segÃºn decisiÃ³n
5. âœ… Optimizar detector de arquetipos
6. âœ… Pruebas de rendimiento (tiempo, tokens)
7. âœ… DocumentaciÃ³n final

---

## ğŸš¨ DECISIONES PENDIENTES:

### 1. Â¿Un prompt o mÃºltiples?
**OpciÃ³n A**: Prompt Ãºnico comprehensivo
- Pros: MÃ¡s rÃ¡pido, menos tokens, contexto completo
- Cons: MÃ¡s complejo, puede ser menos preciso

**OpciÃ³n B**: MÃºltiples prompts especialistas
- Pros: MÃ¡s preciso por patrÃ³n
- Cons: MÃ¡s lento, mÃ¡s tokens, 8 llamadas por archivo

### 2. Â¿CÃ³mo manejar archivos con mÃºltiples patrones?
**OpciÃ³n A**: Un anÃ¡lisis que los detecte todos
**OpciÃ³n B**: MÃºltiples anÃ¡lisis en cola

---

## ğŸ“ TEST CASES FUNCIONANDO:

- `scenario-2-semantic`: 6 archivos, todos procesados con LLM
- Resultado: Todos tienen llmInsights con analysisType: "orphan-module"
- Problema: Razonamientos genÃ©ricos, no especÃ­ficos

---

## ğŸ’¾ COMANDOS ÃšTILES:

```bash
# Limpiar y re-ejecutar
rm -rf test-cases/scenario-2-semantic/.OmnySysData && \
timeout 180 node src/layer-c-memory/mcp/index.js ./test-cases/scenario-2-semantic

# Verificar llmInsights
node --input-type=module -e "
import fs from 'fs';
const data = JSON.parse(fs.readFileSync('./test-cases/scenario-2-semantic/.OmnySysData/files/src/GameStore.js.json', 'utf8'));
console.log('Has llmInsights:', !!data.llmInsights);
console.log('Analysis:', data.llmInsights?.analysisType);
console.log('Reasoning:', data.llmInsights?.reasoning);
"
```

---

## ğŸ”— REFERENCIAS:

- DocumentaciÃ³n LFM2.5-Instruct: Usa ChatML v3, XML tags, CoT
- Arquetipos registrados: 9 tipos en PROMPT_REGISTRY.js
- Prioridades: critical > high > medium > low

---

## ğŸ‘¤ AUTOR: Claude (Opencode)
## ğŸ“ NOTAS: Sistema funcionando, pendiente optimizaciÃ³n de prompts
