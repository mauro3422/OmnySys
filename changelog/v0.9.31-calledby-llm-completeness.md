# v0.9.31 — Cross-file calledBy index + LLM Metadata Completeness Score + OOM Fix

**Date:** 2026-02-19

## Summary

Three improvements that together make the system more self-sufficient and smarter about when to use the LLM.

---

## 1. Cross-file `calledBy` index (Bug Fix)

**File:** `src/layer-a-static/indexer.js` — new Step 3.6

**Root cause:** `buildCallGraph()` ran per-file and only connected atoms within the same file. Cross-file calls (e.g. `core.js` calling `needsLLMAnalysis` from `analysis-decider.js`) were never reflected in `atom.calledBy`, so every atom showed `calledBy: 0` even when widely used.

**Fix:** After all files are extracted (Step 3.5), a global pass builds a `functionName → atom` lookup across ALL atoms from ALL files. For every `external` call in every atom, if the target function is found in any other file, the caller's ID is appended to `targetAtom.calledBy`. Updated atoms are re-saved to disk.

**Result:**
- `get_function_details` now shows correct `calledBy` counts
- `calledBy` accurately drives archetype detection (dead-code vs widely-used)
- Impact maps and risk assessments become more accurate

---

## 2. LLM Activation — Metadata Completeness Score (v0.7)

**File:** `src/layer-b-semantic/llm-analyzer/analysis-decider.js`

**Root cause:** The previous logic used 7 independent boolean checks, each of which could independently trigger LLM. Because many conditions were always true (e.g. `semanticConnections` empty because the connection index wasn't populated yet), the bypass rate was far below the 90% target.

**New approach — `computeMetadataCompleteness(fileAnalysis)`:**

Scores 8 key metadata fields (0–1 scale):
| Field | What it checks |
|---|---|
| `atoms` | File has extracted functions |
| `calledBy` | At least one atom has callers (or nothing exported) |
| `imports` | Has declared imports or is self-contained |
| `exports` | Has at least one export |
| `dataFlow` | At least one atom has input/output flow data |
| `calls` | At least one atom makes calls |
| `semanticConnections` | Events/globals resolved (or none expected) |
| `archetype` | At least one non-standard archetype detected |

**Decision logic:**
```
score >= 0.75  →  skip LLM (Layer A knows this file well)
score < 0.75   →  LLM only if gaps are semantically significant
                  (no-atoms, no-callers, unresolved-semantic, no-dataflow)
dynamic code   →  always LLM (eval/dynamic imports can't be resolved statically)
```

**Result:** Files that are well-understood statically (utility functions, config files, pure re-exports, well-connected modules) skip LLM entirely. LLM activates only for genuinely opaque files.

---

## 3. LLM Queue OOM Fix

**Files:** `src/core/orchestrator/llm-analysis.js`, `src/core/jobs/JobAnalyzer.js`

- Queue jobs no longer carry `fileAnalysis` (was ~10KB per file × 219 files = 2GB+ in queue RAM)
- Worker loads `fileAnalysis` fresh from disk when job starts
- After saving results, `job.fileAnalysis = null` allows immediate GC
- Peak memory: 2 concurrent jobs × ~25MB = ~50MB instead of 3.6GB OOM

---

## Impact

| Metric | Before | After |
|--------|--------|-------|
| `calledBy` on cross-file functions | Always 0 | Correct count |
| LLM bypass rate (estimated) | ~30% | ~75%+ |
| Heap during 219-file LLM queue | 3.6GB → crash | ~50MB |
| LLM JSON truncation | Frequent | Fixed (max_tokens 1000→2048) |
