# v0.9.30 — Bug Fixes: Data Flow via AST Node + Atom ID Normalization + ChainBuilder Stack Overflow

**Date:** 2026-02-19

## Summary

Three bugs diagnosed and fixed that were causing silent metadata loss, corrupted atom IDs, and stack overflows during analysis.

---

## Bug Fixes

### 1. ChainBuilder Stack Overflow on Cyclic Call Graphs
**File:** `src/layer-a-static/pipeline/molecular-chains/builders/ChainBuilder.js`

`buildChainFromAtom` never added the current atom to `visited` before recursing. Any cycle between non-entry atoms (B→C→B) caused infinite recursion → `Maximum call stack size exceeded`.

**Fix:** Added `visited.add(atom.id)` at the start of `buildChainFromAtom`.

---

### 2. Data Flow Extraction Fails for Class Methods (Silent Metadata Loss)
**Files:**
- `src/layer-a-static/extractors/data-flow/index.js`
- `src/layer-a-static/extractors/data-flow/visitors/transformation-extractor/utils/ast-helpers.js`
- `src/layer-a-static/extractors/data-flow/visitors/input-extractor/helpers/ast-helpers.js`
- `src/layer-a-static/extractors/data-flow/visitors/output-extractor/helpers/ast-helpers.js`
- `src/layer-a-static/pipeline/phases/atom-extraction/extraction/atom-extractor.js`

**Root cause:** `extractDataFlow` re-parsed the function code as a standalone program. Class methods (`static foo() {}`) are invalid outside a class body → Babel threw "Unexpected reserved word 'static'" → `dataFlow: null` for all class methods.

**Fix (structural):**
- `extractDataFlow` now accepts a Babel AST node OR a string. If a node is passed, skips re-parsing entirely.
- `atom-extractor` passes `functionInfo.node` (already available from initial parse) instead of re-parsing the string.
- All three `findFunctionNode` helpers now recognize `ClassMethod` and `ObjectMethod` node types directly.

**Result:** Zero "Data flow extraction failed" errors. 5913 atoms extracted cleanly. Class methods now have `inputs`, `transformations`, and `outputs` populated.

---

### 3. Atom IDs Corrupted on Windows (Absolute Path in ID)
**Files:**
- `src/layer-a-static/pipeline/phases/atom-extraction/builders/metadata-builder.js`
- `src/layer-a-static/parser/helpers.js`

**Root cause:** `getFileId(filePath)` received the absolute Windows path (`C:\Dev\OmnySystem\src\...`) and replaced all separators with underscores → IDs like `C_Dev_OmnySystem_src_layerastatic_...::method`. This broke `analyze_change`, `calledBy` lookups, and all graph connections.

**Fix:**
- `metadata-builder.js` now generates the atom ID directly from the relative `filePath` (already normalized at that stage) instead of using `functionInfo.id` from the parser.
- `getFileId` in `helpers.js` now strips the absolute path prefix by detecting project markers (`/src/`, `/lib/`, etc.) before processing.

---

### 4. Debug Terminal Log Accumulation Between Restarts
**File:** `src/layer-c-memory/mcp-server.js`

The proxy only truncated `logs/mcp-server.log` on its own startup (which never happens after initial CLI launch). Subsequent `restart_server` calls accumulated all logs in the terminal window.

**Fix:** `scheduleRestart()` now truncates the log file before spawning each new worker. Active on next CLI restart.

---

## Impact

### 5. OOM Crash — LLM Queue Stores All fileAnalysis Objects in RAM
**Files:** `src/core/orchestrator/llm-analysis.js`, `src/core/jobs/JobAnalyzer.js`

**Root cause:** The LLM analysis pipeline accumulated **all** `fileAnalysis` objects (one per file) into a `filesNeedingLLM` array, then passed them again into each queue job — duplicating ~2GB of data in RAM simultaneously. With 219 files queued, this caused a 3.6GB heap and OOM crash.

**Fix:**
- Removed `fileAnalysis` from `filesNeedingLLM` push and from `queue.enqueueJob()` — queue now stores only `filePath` + `archetypes`
- `JobAnalyzer.runLLMAnalysis()` loads `fileAnalysis` fresh from disk per job (it was already doing a Layer A re-analysis anyway)
- After saving results, `job.fileAnalysis = null` to allow GC of the large object immediately

**Result:** At any moment only 2 `fileAnalysis` objects are in RAM (the 2 concurrent active jobs), not 219.

### 6. LLM JSON Truncated — max_tokens Too Low
**File:** `src/ai/llm/client/analysis/analyzer.js`

**Root cause:** `max_tokens: 1000` caused the local LLM to truncate JSON mid-response, producing parse errors and `confidence: 0`.

**Fix:** Raised to `max_tokens: 2048`.

---

| Metric | Before | After |
|--------|--------|-------|
| Data flow extraction errors | ~300 per full analysis | 0 |
| Class methods with `dataFlow: null` | All | None |
| Atom IDs with absolute Windows path | All class/arrow methods | 0 |
| Stack overflow on cyclic call graphs | Yes | Fixed |
| Heap usage during 219-file LLM queue | ~3.6 GB → OOM crash | ~50 MB (2 jobs × ~25MB) |
| LLM JSON parse errors (truncated) | Frequent | Fixed |
